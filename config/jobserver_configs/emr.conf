spark {
  # spark.master will be passed to each job's JobContext
  master = "yarn-client"
  jobserver {
    port = 8090
    jar-store-rootdir = /mnt/tmp/spark-jobserver/jars
    jobdao = spark.jobserver.io.JobFileDAO
    filedao {
      rootdir = /mnt/tmp/spark-jobserver/filedao/data
    }
  }
  # predefined Spark contexts
  contexts {
    cassandra-context {
      driver-memory = 8G
      driver-core = 4
      spark.cassandra.connection.host = 10.0.1.248
    }
  }
  # universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    num-cpu-cores = 8           # Number of cores to allocate.  Required.
    memory-per-node = 10G          # Executor memory per node, -Xmx style eg 512m, #1G, etc.
  }
  # This needs to match SPARK_HOME for cluster SparkContexts to be created successfully
  home = "/usr/lib/spark"
}
